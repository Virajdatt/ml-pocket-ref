{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4ea87d",
   "metadata": {},
   "source": [
    "## Chapter 3 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa00793",
   "metadata": {},
   "source": [
    "## Leaky features\n",
    "\n",
    "Leakage is also termed Data Leakage\n",
    "\n",
    "Leaky features are variables that contain information about the future or target. There’s nothing bad in having data about the target, and we often have that data during model creation time. However, if those variables are not available when we perform a prediction on a new sample, we should remove them from the model as they are leaking data from the future.\n",
    "\n",
    "<b>Leakage (machine learning)<b>\n",
    "\n",
    "In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.\n",
    "\n",
    "\n",
    "<b>Data Leakage Examples</b>\n",
    "\n",
    "1. Giveaway features: Giveaway features are the features that expose information about the target variable and would not be available after the model is deployed.\n",
    "\n",
    "2. Leakage during preprocessing: If preprocessing is done on both train and test set.\n",
    "\n",
    "<b>How to Detect and Avoid Data Leakage</b>\n",
    "\n",
    "As a general, if the model is too good to be true, we should get suspicious. The model might be somehow memorizing the feature-target relations instead of learning and generalizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c58a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68257db7",
   "metadata": {},
   "source": [
    "[Data Science Project Suggestion](!https://drivendata.github.io/cookiecutter-data-science/)\n",
    "\n",
    "### Classification Problem Flow\n",
    "\n",
    "1. Gather Data\n",
    "2. Clean Data\n",
    "3. Create Features\n",
    "4. Sample Data (split train and test data)\n",
    "5. Impute Data\n",
    "6. Normalize Data / (Refactor code)\n",
    "7. Baseline Model\n",
    "8. Build Classifier\n",
    "9. Stack\n",
    "10. Evaluate Model\n",
    "11. Optimize Model (Hyper-parameter tuning)\n",
    "12. Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee12e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee5cb45c",
   "metadata": {},
   "source": [
    "## Chapter 4 Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979f9f6",
   "metadata": {},
   "source": [
    "To visualize patterns in the missing data, use the missingno library. This library is useful for viewing contiguous areas of missing data, which would indicate that the missing data is not random (see Figure 4-1). The matrix function includes a sparkline along the right side. Patterns here would also indicate non‐ random missing data. You may need to limit the number of samples to be able to see the patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccca69c",
   "metadata": {},
   "source": [
    "A <b>dendrogram</b> can also show missing data it is done by clustering of where data is missing ). Leaves that are at the same level predict one another’s presence (empty or filled). The vertical arms are used to indicate how different clusters are. Short arms mean that branches are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ab513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1854f9d6",
   "metadata": {},
   "source": [
    "## Chapter 5 Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8748d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c891d290",
   "metadata": {},
   "source": [
    "## Important note on pandas behaviour\n",
    "\n",
    "Up to pandas 0.23, if the type is int64, we are guaranteed that there are no missing values. If the type is float64, the values might be all floats, but also could be integer-like numbers with missing values. The pandas library converts integer values that have missing numbers to floats, as this type supports missing values. The object typically means string types (or both string and numeric).\n",
    "\n",
    "\n",
    "As of pandas 0.24, there is a new Int64 type (notice the capitalization). This is not the default integer type, but you can coerce to this type and have support for missing numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ff036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3adf2ac3",
   "metadata": {},
   "source": [
    "## Chapter 6 Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94215b36",
   "metadata": {},
   "source": [
    "A pandas DataFrame has an iloc attribute that we can do index operations on. It will let us pick out rows and columns by index location. We pass in the row positions as a scalar, list, or slice, and then we can add a comma and pass in the column positions as a scalar, list, or slice.\n",
    "Here we pull out the second and fifth row, and the last three columns:\n",
    "<code> X.iloc[[1, 4], -3:] ;sex_male embarked_Q embarked_S\n",
    "    677       1.0           0           1\n",
    "    864       0.0           0           1\n",
    "    \n",
    "</code>\n",
    "There is also a .loc attribute, and we can put out rows and columns based on name (rather than position). Here is the same portion of the DataFrame:\n",
    "<code> X.loc[[677, 864], \"sex_male\":] ;sex_male embarked_Q embarked_S 677 1.0 0 1 864 0.0 0 1\n",
    "\n",
    "</code>\n",
    "\n",
    "\n",
    "###  Joint Plot\n",
    "Yellowbrick has a fancier scatter plot that includes histograms on the edge as well as a regression line called a joint plot\n",
    "\n",
    "### Pair Grid\n",
    "Using seaborn\n",
    "\n",
    "Kernel Density Estimations:\n",
    "- https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV0405/MISHRA/kde.html\n",
    "- https://www.youtube.com/watch?v=x5zLaWT5KPs\n",
    "\n",
    "### Pandas chaining sample\n",
    "\n",
    "\n",
    "### Correlation/Covarince correlation graph (yellowbrick and seaborn)\n",
    "\n",
    "### Heatmap\n",
    "\n",
    "### RadViz (yellowbricks/Pandas as implements RadViz Plot)\n",
    "\n",
    "A RadViz plot shows each sample on a circle, with the features on the circumference. The values are normalized, and you can imagine that each figure has a spring that pulls samples to it based on the value.\n",
    "This is one technique to visualize separability between the targets.\n",
    "\n",
    "\n",
    "\n",
    "### Parallel Coordiantes (yellowbricks/Pandas as implements RadViz Plot)\n",
    "\n",
    "For multivariate data, you can use a parallel coordinates plot to see clustering visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea68849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbfde298",
   "metadata": {},
   "source": [
    "## Chapter 7 Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc0721",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "- StandardScaler\n",
    "- MinMaxScaler\n",
    "### Categorical data encoding\n",
    "- Frequency Encoding (code)\n",
    "- Pulling Categories from Strings: Use the strings to create some kind of category. In titanic dataset use the beggining of name to identify the designation (Mr, Miss, Master, Countess)\n",
    "- OrdinalEncoder: Use the pandas map function to map ordinal levels map = {'high':2, 'med':1, 'low':0}\n",
    "\n",
    "### Feature Engg\n",
    "- fastai add_datepart (careful, mutates the actual dataframe)\n",
    "- Do Manual Feature Engineering based on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007cae6",
   "metadata": {},
   "source": [
    "## Chapter 8 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266df82",
   "metadata": {},
   "source": [
    "### Collinear Columns\n",
    "- using the rfpimp.plot_dependence_heatmap\n",
    "\n",
    "### Lasso -Regression use it's alpha parameter to define important features \n",
    "- didn't quite understand how to this works\n",
    "\n",
    "### Recursive Feature Elimination using yellow-bricks\n",
    "\n",
    "### Mutual Information: \n",
    "- A non-parametric test using k-nearest neighbours \n",
    "- Mutual information quantifies the amount of information gained by observing another variable. The value is zero or more. If the value is zero, then there is no relation between them. This number is not bounded and represents the number of bits shared between the feature and the target\n",
    "\n",
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c7dab",
   "metadata": {},
   "source": [
    "# Chapter 9: Imbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2b247",
   "metadata": {},
   "source": [
    "### Tree-based Algorithms and Ensembles: \n",
    "- May perform well if the classes tend to cluster\n",
    "\n",
    "### Penalize Models\n",
    "- sklearn: classification models support the class_weight parameter Setting this to 'balanced' will attempt to regularize minority classes and incentivize the model to clas‐ sify them correctly. Alternatively, you can grid search and spec‐ ify the weight options by passing in a dictionary mapping class to weight \n",
    "\n",
    "------------\n",
    "\n",
    "-  XGBoost: library has the max_delta_step parameter, which can be set from 1 to 10 to make the update step more conserva‐ tive. It also has the scale_pos_weight parameter that sets the ratio of negative to positive samples (for binary classes). Also, the eval_metric should be set to 'auc' rather than the default value of 'error' for classification.\n",
    "------------\n",
    "- KNN: model has a weights parameter that can bias neigh‐ bors that are closer. If the minority class samples are close together, setting this parameter to 'distance' may improve performance.\n",
    "\n",
    "### Upsampling Minority\n",
    "- sklearn.utils import resample\n",
    "- imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "### Generate Minority Data\n",
    "- SMOTE, ADA-SYN\n",
    "\n",
    "\n",
    "### Downsampling Majority\n",
    "- sklearn.utils import resample\n",
    "\n",
    "### Upsampling Then Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b58c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd078626e5cb14307b2371f3946c1caa5bbc168a9eda33df78f60dc6faeb16eeee6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
